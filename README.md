# StubHub Scraper

A fast, faultâ€‘tolerant CLI utility that harvests event data from StubHubâ€™s *Explore* API for **every city** listed in `worldcities.csv`, combining the results into a single, tidy `events.csv`â€”all while automatically resuming where it left off whenever you restart the script.

---

## âœ¨ Key Features

| Feature                | Description                                                                                                                                         |
| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Consolidated CSV**   | Writes all events to `events.csv` with a fixed column order thatâ€™s ready for analysis in Excel, Pandas, SQL, etc.                                   |
| **Resumeâ€‘fromâ€‘log**    | Progress for each city (or a *done* flag) is appended to `progress.log`.  Restart the script anytime and it will pick up exactly where it left off. |
| **Parallel Scraping**  | Processes up to **`CONCURRENT`** cities at once using Pythonâ€™s `ThreadPoolExecutor` for faster overall runtime.                                     |
| **Rateâ€‘limit Safety**  | Adds a oneâ€‘second delay (`WAIT_SECS`) between page requests *per city* to avoid hammering the API.                                                  |
| **Outâ€‘file Cache**     | Raw JSON from every page is stored under `wget_output/` so you can reâ€‘parse or audit later.                                                         |
| **Threadâ€‘safe Writes** | Exclusive locks protect both `events.csv` and `progress.log`, preventing race conditions in parallel mode.                                          |
| **403 Tip for Mobile** | Detects HTTPÂ 403 (IPÂ blocked) and prints an *Airplaneâ€‘mode* toggle tipâ€”handy when running on Termux or behind a changing mobile IP.                 |
| **Maxâ€‘page Guard**     | `MAX_PAGES` caps requests per city to stay within sane limits if StubHub ever loops or paginates infinitely.                                        |
| **Emptyâ€‘page Prune**   | If a page returns an empty `"events": []`, the city is marked complete and the temporary JSON file is deleted.                                      |

---

## ðŸ—‚ File / Folder Structure

```
project/
â”œâ”€â”€ stubhub_scraper.py      # â† this script
â”œâ”€â”€ worldcities.csv         # Source city list (lat/lon)
â”œâ”€â”€ events.csv              # â¬…ï¸ Consolidated output (autoâ€‘created)
â”œâ”€â”€ progress.log            # â¬…ï¸ Resume checkpoint (autoâ€‘created)
â”œâ”€â”€ wget_output/            # Raw API JSON (one file per page)
â””â”€â”€ README.md               # â† you are here
```

---

## ðŸ”§ Prerequisites

* **PythonÂ 3.8+**  (tested on 3.10)
* **wget** commandâ€‘line tool
* **pip packages**: `pandas`  (install via `pip install -r requirements.txt` or manually)
* A stable Internet connection ðŸš€

> **Tip (Linux / Termux):** Use a VPN or rotating IP address to minimise 403s.

---

## âš™ï¸ Configuration

Open `stubhub_scraper.py` and edit the *CONFIG* block at the top if needed:

```python
INPUT_CSV     = "worldcities.csv"   # Source of cities
OUT_DIR       = Path("wget_output") # Folder for raw JSON
COMBINED_CSV  = "events.csv"        # Final merged file
PROGRESS_LOG  = "progress.log"      # Resume checkpoint

CONCURRENT    = 5    # Parallel city workers
WAIT_SECS     = 1    # Delay between page requests
TOP_N         = None # Limit number of cities (None = all)
MAX_PAGES     = 100  # Safety limit per city
```

*For most users, the defaults are fine.*  Increase `CONCURRENT` cautiouslyâ€”more threads mean more sockets (and a higher chance of getting blocked).

---

## ðŸš€ QuickÂ Start

```bash
# 1. Clone or copy the repo
$ git clone https://github.com/yourname/stubhub-scraper.git
$ cd stubhub-scraper

# 2. Install Python deps
$ pip install pandas

# 3. Run the scraper
$ python stubhub_scraper.py
```

Progress is displayed in real time, for example:

```
ðŸŒ United States, New York (start at page 0)
ðŸ”— Fetching page 0...
âœ” Saved 50 events from page 0
ðŸ”— Fetching page 1...
...
```
![image](https://github.com/user-attachments/assets/4bead2ef-4e74-40de-b1da-36416ab5a874)

> **Interruption?** Press **Ctrlâ€‘C** at any time. When you restart, the script uses `progress.log` to resume.

---

## â™»ï¸ Resuming & Restarting

* **Completed cities** are tagged `done` in `progress.log`.
* **Inâ€‘progress cities** store the *next* page number to fetch.
* **Automatic cleanup**: When *all* cities show `done`, the script deletes `progress.log` and prints an *AllÂ done* message.

---

## ðŸ›‘ Handling 403 Errors

StubHub sometimes blocks an IP after \~100â€“150 requests. The script watches `wget`â€™s exit code / stderr for `403Â Forbidden`:

> **Recommendation:** If you encounter a 403, it's best to run this script locally on a Unix-based system (Linux/macOS/Termux) for more control over your IP.
>
> â€¢ **On a Wi-Fi router**: Restart your router to obtain a new IP address.
>
> â€¢ **Using mobile hotspot or Termux**: Toggle airplane mode OFF and ON to get a fresh IP.

1. It prints a red âš ï¸ message.
2. Suggests toggling **AirplaneÂ mode** (mobile) or reconnecting your router/VPN.
3. Continues quietly until you regain access.

*Nothing is lostâ€”just rerun the script and it resumes automatically.*

---

## ðŸ—ƒ Output Schema (`events.csv`)

| Column                                             | Meaning                                       |
| -------------------------------------------------- | --------------------------------------------- |
| city, country, page                                | Where & from which page the event was scraped |
| eventId, name, url                                 | StubHub event metadata                        |
| dayOfWeek, formattedDateWithoutYear, formattedTime | Humanâ€‘readable schedule                       |
| venueName, formattedVenueLocation, venueId         | Venue details                                 |
| categoryId, imageUrl                               | Event category & hero image                   |
| priceClass, isUnderHundred                         | Price hints                                   |
| isTbd, isDateConfirmed, isTimeConfirmed            | Date/time certainty flags                     |
| eventState, hasActiveListings, aggregateFavorites  | Live state & popularity                       |
| isFavorite, isParkingEvent, isRefetchedGlobalEvent | Misc flags                                    |

The columns are **always in the same order**, perfect for Pandas `df = pd.read_csv("events.csv")`.

---

## ðŸ›  Customisation Ideas

* **Rotate Proxies** â€“ Feed a list of proxies to `wget` via `--execute use_proxy=yes`.
* **Change Headers** â€“ Edit the `HEADERS` list for a different browser fingerprint.
* **Alternative Downloader** â€“ Swap `wget` for `curl` or `requests` if you prefer.
* **Database Sink** â€“ Stream parsed rows straight to Postgres/MySQL instead of a CSV.

---

## ðŸ¤– Troubleshooting

| Symptom                                        | Cause / Fix                                                        |
| ---------------------------------------------- | ------------------------------------------------------------------ |
| `ERRORÂ 403: Forbidden`                         | IP blocked â€“ change IP, wait 15â€“20Â min, then restart.              |
| `No such file or directory: 'worldcities.csv'` | Ensure the CSV is in the script folder.                            |
| Hangs at startup                               | Check Internet; DNS or network issue.                              |
| Too slow                                       | Increase `CONCURRENT` or decrease `WAIT_SECS`, but risk more 403s. |

---

## ðŸ“„ License

MIT â€“ do what you want, but donâ€™t sue the author.

---

## âš ï¸ Disclaimer

This script is provided for educational and research purposes only. The developer is **not responsible** for any misuse, damages, or violations of any serviceâ€™s terms of use, including StubHub. Use at your own risk.

---

## ðŸ™Œ Acknowledgements

* [StubHub](https://www.stubhub.com/) â€“ for the data (respect their ToS).
* *worldcities.csv* by SimpleMaps â€“ city lat/lon dataset.

> Pull requests welcome!  Happy scraping.
